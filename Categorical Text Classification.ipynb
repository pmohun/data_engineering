{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Text into Categories using Natural Language API\n",
    "\n",
    "### Overview: \n",
    "The Cloud Natural Language API lets you extract entities from text, perform sentiment and syntactic analysis, and classify text into categories. In this lab, we'll focus on text classification. Using a database of 700+ categories, this API feature makes it easy to classify a large dataset of text.\n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "- Creatie a Natural Language API request and calling the API with curl\n",
    "- Use the NL API's text classification feature\n",
    "- Use text classification to understand a dataset of news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Generate and save API key. Using Cloud Shell:\n",
    "\n",
    "```\n",
    "export API_KEY=<YOUR_API_KEY>\n",
    "```\n",
    "\n",
    "2.) Classify a news article\n",
    "\n",
    "- Using the Natural Language API's classifyText method, you can sort text data into categories with a single API call. This method returns a list of content categories that apply to a text document. These categories range in specificity, from broad categories like /Computers & Electronics to highly specific categories such as /Computers & Electronics/Programming/Java (Programming Language). A full list of 700+ possible categories can be found here.\n",
    "\n",
    "- We'll start by classifying a single article, and then we'll see how we can use this method to make sense of a large news corpus. To start, let's take this headline and description from a New York Times article in the food section:\n",
    "\n",
    "```\"A Smoky Lobster Salad With a Tapa Twist. This spin on the Spanish pulpo a la gallega skips the octopus, but keeps the sea salt, olive oil, pimentón and boiled potatoes.\"```\n",
    "\n",
    "- In your Cloud Shell environment, create a request.json file with the code below. You can either create the file using one of your preferred command line editors (nano, vim, emacs) or use the Cloud Shell code editor (click the pencil button):\n",
    "\n",
    "```\n",
    "{\n",
    "  \"document\":{\n",
    "    \"type\":\"PLAIN_TEXT\",\n",
    "    \"content\":\"A Smoky Lobster Salad With a Tapa Twist. This spin on the Spanish pulpo a la gallega skips the octopus, but keeps the sea salt, olive oil, pimentón and boiled potatoes.\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "- Save\n",
    "```\n",
    "curl \"https://language.googleapis.com/v1/documents:classifyText?key=${API_KEY}\" \\\n",
    "  -s -X POST -H \"Content-Type: application/json\" --data-binary @request.json\n",
    "```\n",
    "\n",
    "\n",
    "3.) Classifying a large text dataset\n",
    "\n",
    "- To see how the classifyText method can help us understand a dataset with lots of text, you'll use this public dataset of BBC news articles. The dataset consists of 2,225 articles in five topic areas (business, entertainment, politics, sports, tech) from 2004 - 2005. A subset of these articles are in a public Google Cloud Storage bucket. Each of the articles is in a .txt file.\n",
    "\n",
    "- To examine the data and send it to the Natural Language API, you'll write a Python script to read each text file from Cloud Storage, send it to the classifyText endpoint, and store the results in a BigQuery table. BigQuery is Google Cloud's big data warehouse tool - it lets you easily store and analyze large data sets.\n",
    "\n",
    "- To see the type of text you'll be working with, run the following command to view one article (gsutil provides a command line interface for Cloud Storage):\n",
    "\n",
    "```\n",
    "gsutil cat gs://text-classification-codelab/bbc_dataset/entertainment/001.txt\n",
    "```\n",
    "\n",
    "4.) Creating a BigQuery table for our categorized text data\n",
    "\n",
    "- Before sending the text to the Natural Language API, you need a place to store the text and category for each article.\n",
    "- Navigate to the BigQuery in the Console. You will need to log in - remember to use your Qwiklabs credentials\n",
    "- Select Create new datase and name dataset: news_classification_dataset. Leave the defaults in the Data location and Data expiration fields.\n",
    "- Click on the dropdown arrow next to your dataset name and select Create new table. Under Source Data, select \"Create empty table\". Then name your table article_data and add the following 3 fields in the schema: article_text, category, and confidence.\n",
    "\n",
    "5.) Classifying news data and storing the result in BigQuery\n",
    "\n",
    "- Before writing a script to send the news data to the Natural Language API, you need to create a service account. This will be used to authenticate to the Natural Language API and BigQuery from a Python script.\n",
    "- First, back in Cloud Shell, export the name of your Cloud project as an environment variable. Replace <your_project_name> with the GCP Project ID found in the CONNECTION DETAILS section of the lab:\n",
    "\n",
    "```\n",
    "export PROJECT=<your_project_name>\n",
    "\n",
    "# from cloud shell\n",
    "gcloud iam service-accounts create my-account --display-name my-account\n",
    "gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:my-account@$PROJECT.iam.gserviceaccount.com --role=roles/bigquery.admin\n",
    "gcloud iam service-accounts keys create key.json --iam-account=my-account@$PROJECT.iam.gserviceaccount.com\n",
    "export GOOGLE_APPLICATION_CREDENTIALS=key.json\n",
    "```\n",
    "\n",
    "- Now you're ready to send the text data to the Natural Language API!\n",
    "- To do that, write a Python script using the Python module for Google Cloud. You can accomplish the same thing from any language, there are many different cloud client libraries.\n",
    "- Create a file called classify-text.py and copy the following into it. Replace YOUR_PROJECT with your GCP Project ID.\n",
    "\n",
    "```\n",
    "from google.cloud import storage, language, bigquery\n",
    "\n",
    "# Set up our GCS, NL, and BigQuery clients\n",
    "storage_client = storage.Client()\n",
    "nl_client = language.LanguageServiceClient()\n",
    "# TODO: replace YOUR_PROJECT with your project id below\n",
    "bq_client = bigquery.Client(project='YOUR_PROJECT')\n",
    "\n",
    "dataset_ref = bq_client.dataset('news_classification_dataset')\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "table_ref = dataset.table('article_data') # Update this if you used a different table name\n",
    "table = bq_client.get_table(table_ref)\n",
    "\n",
    "# Send article text to the NL API's classifyText method\n",
    "def classify_text(article):\n",
    "        response = nl_client.classify_text(\n",
    "                document=language.types.Document(\n",
    "                        content=article,\n",
    "                        type=language.enums.Document.Type.PLAIN_TEXT\n",
    "                )\n",
    "        )\n",
    "        return response\n",
    "\n",
    "rows_for_bq = []\n",
    "files = storage_client.bucket('text-classification-codelab').list_blobs()\n",
    "print(\"Got article files from GCS, sending them to the NL API (this will take ~2 minutes)...\")\n",
    "\n",
    "# Send files to the NL API and save the result to send to BigQuery\n",
    "for file in files:\n",
    "        if file.name.endswith('txt'):\n",
    "                article_text = file.download_as_string()\n",
    "                nl_response = classify_text(article_text)\n",
    "                if len(nl_response.categories) > 0:\n",
    "                        rows_for_bq.append((article_text, nl_response.categories[0].name, nl_response.categories[0].confidence))\n",
    "\n",
    "print(\"Writing NL API article data to BigQuery...\")\n",
    "# Write article text + category data to BQ\n",
    "errors = bq_client.insert_rows(table, rows_for_bq)\n",
    "assert errors == []\n",
    "```\n",
    "- Run:\n",
    "\n",
    "```\n",
    "python classify-text.py\n",
    "\n",
    "```\n",
    "\n",
    "#### We're using the google-cloud Python client library to access Cloud Storage, the Natural Language API, and BigQuery. First, a client is created for each service; then references are created to the BigQuery table. files is a reference to each of the BBC dataset files in the public bucket. We iterate through these files, download the articles as strings, and send each one to the Natural Language API in our classify_text function. For all articles where the Natural Language API returns a category, the article and its category data are saved to a rows_for_bq list. When classifying each article is done, the data is inserted into BigQuery using insert_rows().\n",
    "\n",
    "- Navigate to the article_data table in the BigQuery tab and click Query Table. Edit the results in the New Query box so it looks like this, replacing the brackets with a backtick (on the tilde key, next to the 1) not a single quote, adding an asterisk between SELECT and FROM, and replacing the : after your project name with a ‘ .\n",
    "\n",
    "```\n",
    "SELECT * FROM YOUR_PROJECT.news_classification_dataset.article_data\n",
    "\n",
    "```\n",
    "\n",
    "- Click the Show Options button and uncheck the Use Legacy SQL box.\n",
    "- Run Query.\n",
    "- Review data when the query completes. Scroll to the right to see the category column.\n",
    "\n",
    "6.) Analyzing categorized news data in BigQuery\n",
    " \n",
    " - In Big Query: \n",
    " \n",
    "```\n",
    "SELECT\n",
    "     category,\n",
    "      COUNT(*) c\n",
    "FROM\n",
    "    `YOUR_PROJECT.news_classification_dataset.article_data`\n",
    "GROUP BY\n",
    "    category\n",
    "ORDER BY\n",
    "    c DESC   \n",
    "```\n",
    "\n",
    "- To find additional categories:\n",
    "\n",
    "```\n",
    "SELECT * FROM `YOUR_PROJECT.news_classification_dataset.article_data`\n",
    "WHERE category = \"/Arts & Entertainment/Music & Audio/Classical Music\"\n",
    "```\n",
    "\n",
    "- High confidence only:\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  article_text,\n",
    "  category\n",
    "FROM `YOUR_PROJECT.news_classification_dataset.article_data`\n",
    "WHERE cast(confidence as float64) > 0.9\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
